{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XjaQuuS2QVQ"
   },
   "source": [
    "# Implementing A Logistic Regression Model from Scratch with PyTorch\n",
    "\n",
    "![altÂ text](https://drive.google.com/uc?export=view&id=11Bv3uhZtVgRVYVWDl9_ZAYQ0GU36LhM9)\n",
    "\n",
    "\n",
    "In this tutorial, we are going to implement a logistic regression model from scratch with PyTorch. The model will be designed with neural networks in mind and will be used for a simple image classification task. I believe this is a great approach to begin understanding the fundamental building blocks behind a neural network. Additionally, we will also look at best practices on how to use PyTorch for training neural networks.\n",
    "\n",
    "After completing this tutorial the learner is expected to know the basic building blocks of a logistic regression model. The learner is also expected to apply the logistic regression model to a binary image classification problem of their choice using PyTorch code.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Elvis Saravia ( [Twitter](https://twitter.com/omarsar0) | [LinkedIn](https://www.linkedin.com/in/omarsar/))\n",
    "\n",
    "**Complete Code Walkthrough:** [Blog post](https://medium.com/dair-ai/implementing-a-logistic-regression-model-from-scratch-with-pytorch-24ea062cd856?source=friends_link&sk=49dcddb17d1d021d2d677f3666c88463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8047,
     "status": "ok",
     "timestamp": 1648995957182,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "6gnyYNkr2Vub"
   },
   "outputs": [],
   "source": [
    "## Import the usual libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1648995960102,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "L7Boavtx22CS",
    "outputId": "f01d483a-55bd-4f47-f612-3cd639eb3013"
   },
   "outputs": [],
   "source": [
    "## configuration to detect cuda or cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSpmMANNj5Uz"
   },
   "source": [
    "## Importing Dataset\n",
    "In this tutorial we will be working on an image classification problem. You can find the public dataset [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip). \n",
    "\n",
    "The objective of our model is to learn to classify between \"bee\" vs. \"no bee\" images.\n",
    "\n",
    "Uncomment the code below to download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1558,
     "status": "ok",
     "timestamp": 1648995962823,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "6tcFUZjHeY0Z",
    "outputId": "0b1c126a-c367-43dd-b7e9-83c2018a7031"
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "!unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doUww-u_37dw"
   },
   "source": [
    "## Data Transformation\n",
    "This is an image classification task, which means that we need to perform a few transformations on our dataset before we train our models. I used similar transformations as used in this [tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#transfer-learning-for-computer-vision-tutorial). For a detailed overview of each transformation take a look at the official torchvision [documentation](https://pytorch.org/docs/stable/torchvision/transforms.html).\n",
    "\n",
    "The following code block performs the following operations:\n",
    "- The `data_transforms` contains a series of transformations that will be performed on each image found in the dataset. This includes cropping the image, resizing the image, converting it to tensor, reshaping it, and normalizing it. \n",
    "- Once those transformations have been defined, then the `DataLoader` function is used to automatically load the datasets and perform any additional configuration such as shuffling, batches, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1648995967412,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "501gdjiu6v24"
   },
   "outputs": [],
   "source": [
    "# configure root folder on your gdrive\n",
    "data_dir = 'hymenoptera_data'\n",
    "\n",
    "# custom transformer to flatten the image tensors\n",
    "class ReshapeTransform:\n",
    "    def __init__(self, new_size):\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        result = torch.reshape(img, self.new_size)\n",
    "        return result\n",
    "\n",
    "# transformations used to standardize and normalize the datasets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        ReshapeTransform((-1,)) # flattens the data\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        ReshapeTransform((-1,)) # flattens the data\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# load the correspoding folders\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# load the entire dataset; we are not using minibatches here\n",
    "train_dataset = torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                            batch_size=len(image_datasets['train']),\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.DataLoader(image_datasets['val'],\n",
    "                                           batch_size=len(image_datasets['val']),\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648995969698,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "OZ3r9BfLp9pH",
    "outputId": "9a0dc4bc-0859-48c6-8078-8b59f11a15eb"
   },
   "outputs": [],
   "source": [
    "len(image_datasets['train']), len(image_datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP9zr48w9xZ3"
   },
   "source": [
    "## Print sample\n",
    "It's always a good practise to take a quick look at the dataset before training your models. Below we print out an example of one of the images from the `train_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 2578,
     "status": "ok",
     "timestamp": 1648995976272,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "BY3MOpT-5U4Q",
    "outputId": "00f1257f-de98-4ad8-8b2a-453083246019"
   },
   "outputs": [],
   "source": [
    "# load the entire dataset\n",
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "# print one example\n",
    "dim = x.shape[1]\n",
    "print(\"Dimension of image:\", x.shape, \"\\n\", \n",
    "      \"Dimension of labels\", y.shape)\n",
    "\n",
    "plt.imshow(x[160].reshape(1, 3, 224, 224).squeeze().T.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNKG-_uLZtQ7",
    "tags": [
     "same_step_1"
    ]
   },
   "source": [
    "## Building the Model\n",
    "Let's now implement our [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) model. Logistic regression is one in a family of machine learning techniques that are used to train binary classifiers. They are also a great way to understand the fundamental building blocks of neural networks, thus they can also be considered the simplest of neural networks where the model performs a `forward` and `backward` propagation to train the model on the data provided. \n",
    "\n",
    "If you don't fully understand the structure of the code below, I strongly recommend you to read the following [tutorial](https://medium.com/dair-ai/pytorch-1-2-introduction-guide-f6fa9bb7597c), which I wrote for PyTorch beginners. You can also check out [Week 2](https://www.coursera.org/learn/neural-networks-deep-learning/home/week/2) of Andrew Ng's Deep Learning Specialization course for all the explanation, intuitions, and details of the different parts of the neural network such as the `forward`, `sigmoid`, `backward`, and `optimization` steps. \n",
    "\n",
    "In short:\n",
    "- The `__init__` function initializes all the parameters (`W`, `b`, `grad`) that will be used to train the model through backpropagation. \n",
    "- The goal is to learn the `W` and `b` that minimimizes the cost function which is computed as seen in the `loss` function below.\n",
    "\n",
    "Note that this is a very detailed implementation of a logistic regression model so I had to explicitly move a lot of the computations into the GPU for faster calcuation, `to(device)` takes care of this in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1648995986435,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "lH1_IRKwR8Zm"
   },
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, dim, lr=torch.scalar_tensor(0.01)):\n",
    "        super(LR, self).__init__()\n",
    "        # intialize parameters\n",
    "        self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)\n",
    "        self.b = torch.scalar_tensor(0).to(device)\n",
    "        self.grads = {\"dw\": torch.zeros(dim, 1, dtype=torch.float).to(device),\n",
    "                      \"db\": torch.scalar_tensor(0).to(device)}\n",
    "        self.lr = lr.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute forward\n",
    "        z = torch.mm(self.w.T, x) + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # compute sigmoid\n",
    "        return 1/(1 + torch.exp(-z))\n",
    "\n",
    "    def backward(self, x, yhat, y):\n",
    "        # compute backward\n",
    "        self.grads[\"dw\"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T)\n",
    "        self.grads[\"db\"] = (1/x.shape[1]) * torch.sum(yhat - y)\n",
    "    \n",
    "    def optimize(self):\n",
    "        # optimization step\n",
    "        self.w = self.w - self.lr * self.grads[\"dw\"]\n",
    "        self.b = self.b - self.lr * self.grads[\"db\"]\n",
    "\n",
    "## utility functions\n",
    "def loss(yhat, y):\n",
    "    m = y.size()[1]\n",
    "    return -(1/m)* torch.sum(y*torch.log(yhat) + (1 - y)* torch.log(1-yhat))\n",
    "\n",
    "def predict(yhat, y):\n",
    "    y_prediction = torch.zeros(1, y.size()[1])\n",
    "    for i in range(yhat.size()[1]):\n",
    "        if yhat[0, i] <= 0.5:\n",
    "            y_prediction[0, i] = 0\n",
    "        else:\n",
    "            y_prediction[0, i] = 1\n",
    "    return 100 - torch.mean(torch.abs(y_prediction - y)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8SZXITgS5sQ"
   },
   "source": [
    "## Pretesting the Model\n",
    "It is also good practice to test your model and make sure the right steps are taking place before training the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2056,
     "status": "ok",
     "timestamp": 1648997746109,
     "user": {
      "displayName": "Elvis Saravia",
      "userId": "17830731333114781815"
     },
     "user_tz": -60
    },
    "id": "L40JX-aXS3cP",
    "outputId": "4697f480-251a-489a-be84-394ad0fc6d4f"
   },
   "outputs": [],
   "source": [
    "# model pretesting\n",
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "# flatten/transform the data\n",
    "x_flatten = x.T\n",
    "y = y.unsqueeze(0) \n",
    "\n",
    "# num_px is the dimension of the images\n",
    "dim = x_flatten.shape[0]\n",
    "\n",
    "# model instance\n",
    "model = LR(dim)\n",
    "model.to(device)\n",
    "yhat = model.forward(x_flatten.to(device))\n",
    "yhat = yhat.data.cpu()\n",
    "\n",
    "# calculate loss\n",
    "cost = loss(yhat, y)\n",
    "prediction = predict(yhat, y)\n",
    "print(\"Cost: \", cost)\n",
    "print(\"Accuracy: \", prediction)\n",
    "\n",
    "# backpropagate\n",
    "model.backward(x_flatten.to(device), yhat.to(device), y.to(device))\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJiwRC7ecBBw",
    "tags": [
     "same_step_2"
    ]
   },
   "source": [
    "## Train the Model\n",
    "It's now time to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4pS54kMTT0n",
    "outputId": "d884dcba-bcfe-43c1-be03-2ac8185d673e",
    "tags": [
     "same_step_4"
    ]
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "costs = []\n",
    "dim = x_flatten.shape[0]\n",
    "learning_rate = torch.scalar_tensor(0.0001).to(device)\n",
    "num_iterations = 500\n",
    "lrmodel = LR(dim, learning_rate)\n",
    "lrmodel.to(device)\n",
    "\n",
    "# transform the data\n",
    "def transform_data(x, y):\n",
    "    x_flatten = x.T\n",
    "    y = y.unsqueeze(0) \n",
    "    return x_flatten, y \n",
    "\n",
    "# train the model\n",
    "for i in range(num_iterations):\n",
    "    x, y = next(iter(train_dataset))\n",
    "    test_x, test_y = next(iter(test_dataset))\n",
    "    x, y = transform_data(x, y)\n",
    "    test_x, test_y = transform_data(test_x, test_y)\n",
    "\n",
    "    # forward\n",
    "    yhat = lrmodel.forward(x.to(device))\n",
    "    cost = loss(yhat.data.cpu(), y)\n",
    "    train_pred = predict(yhat, y)\n",
    "        \n",
    "    # backward\n",
    "    lrmodel.backward(x.to(device), \n",
    "                    yhat.to(device), \n",
    "                    y.to(device))\n",
    "    lrmodel.optimize()\n",
    "\n",
    "    # test\n",
    "    yhat_test = lrmodel.forward(test_x.to(device))\n",
    "    test_pred = predict(yhat_test, test_y)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        costs.append(cost)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Cost after iteration {}: {} | Train Acc: {} | Test Acc: {}\".format(i, \n",
    "                                                                                    cost, \n",
    "                                                                                    train_pred,\n",
    "                                                                                    test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN5v7F1h1uuz"
   },
   "source": [
    "## Result\n",
    "From the loss curve below you can see that the model is sort of learning to classify the images given the decreas in the loss. I only ran the model for `100` iterations. Train the model for many more rounds and analyze the results. In fact, I have suggested a couple of experiments and exercises at the end of the tutorial that you can try to get a more improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sN-m0_a8mx8Z",
    "outputId": "0e497a86-39c1-49e8-d24f-dc553a13ca0e"
   },
   "outputs": [],
   "source": [
    "## the trend in the context of loss\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lsfjo1DQLQBJ"
   },
   "source": [
    "## Some Notes\n",
    "There are many improvements and different experiments that you can perform on top of this notebook to keep practising ML:\n",
    "- It is always good to normalize/standardize your images which helps with learning. As an experiment, you can research and try different ways to standarize the dataset. We have normalized the dataset with the builtin PyTorch [normalizer](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize) which uses the mean and standard deviation. Play around with different transformations or normalization techniques. What effect does this have on learning in terms of speed and loss?\n",
    "- You can try many things to help with learning such as playing around with the learning rate. Try to decrease and increase the learning rate and observe the effect of this in learning? \n",
    "- If you explored the dataset further, you may have noticed that all the \"no-bee\" images are actually \"ant\" images. If you would like to create a more robust model, you may want to make your \"no-bee\" images more random and diverse through some data augmentation technique. This is a more advanced approach but there is a lot of good content to try out this idea. \n",
    "- The model is not really performing well just using simple logistic regression model. It could be because of the dataset I am using and because I didn't train it for long enough. Hyperparameters may also be off. It is a relatively small dataset but the performance could get better with more data and training over time. A more challenging task involves adopting the model to other datasets. Give it a try!\n",
    "- Another important part that is missing in this tutorial is the comprehensive analysis of the model results. If you understand the code, it should be easy to figure out how to test with a few examples. In fact, it would also be great if you can put aside a small testing dataset for this part of the exercise, so as to test the generalization capabilities of the model.\n",
    "- We built the logistic regression model from scratch but with libraries like PyTorch, these days you can simply leverage the high-level functions that implement certain parts of the neural network for you. This simplifies your code and minimizes the amount of bugs in your code. Plus you don't have to code your neural networks from scratch all the time. As a bonus exercise, try to adapt PyTorch builtin modules and functions for implementing a simpler, more concise version of the above logistic regression model. I will also add this as a to-do task for myself and post a solution soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXs3Nx0BIQYZ"
   },
   "source": [
    "## References\n",
    "- [Understanding the Impact of Learning Rate on Neural Network Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)\n",
    "- [Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#transfer-learning-for-computer-vision-tutorial)\n",
    "- [Deep Learning Specialization by Andrew Ng](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch Logistic Regression from Scratch.ipynb",
   "provenance": [
    {
     "file_id": "1AWXvwvyoOczCugTTULLbIPYIh2_GS2Aq",
     "timestamp": 1648335664258
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
