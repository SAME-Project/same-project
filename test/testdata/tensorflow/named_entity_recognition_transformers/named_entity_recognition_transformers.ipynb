{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8qFH7JQE4ht"
   },
   "source": [
    "<a href=\"https://www.kaggle.com/code/ritvik1909/named-entity-recognition-attention\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djp2XOqO88qw"
   },
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "NER is an information extraction technique to identify and classify named entities in text. These entities can be pre-defined and generic like location names, organizations, time and etc, or they can be very specific like the example with the resume.\n",
    "\n",
    "The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type.\n",
    "\n",
    "Named entity recognition is a task that is well-suited to the type of classifier-based approach. In particular, a tagger can be built that labels each word in a sentence using the IOB format, where chunks are labelled by their appropriate type.\n",
    "\n",
    "The IOB Tagging system contains tags of the form:\n",
    "\n",
    "* B - {CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "* I - {CHUNK_TYPE} – for words Inside the chunk\n",
    "* O – Outside any chunk\n",
    "\n",
    "## Approaches to NER\n",
    "* **Classical Approaches:** mostly rule-based.\n",
    "* **Machine Learning Approaches:** there are two main methods in this category: \n",
    "    * Treat the problem as a multi-class classification where named entities are our labels so we can apply different classification algorithms. The problem here is that identifying and labeling named entities require thorough understanding of the context of a sentence and sequence of the word labels in it, which this method ignores that.\n",
    "    * Conditional Random Field (CRF) model. It is a probabilistic graphical model that can be used to model sequential data such as labels of words in a sentence. The CRF model is able to capture the features of the current and previous labels in a sequence but it cannot understand the context of the forward labels; this shortcoming plus the extra feature engineering involved with training a CRF model, makes it less appealing to be adapted by the industry.\n",
    "* **Deep Learning Approaches:** Bidirectional RNNs, Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4267,
     "status": "ok",
     "timestamp": 1650629063227,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "zy7PRVmH7ssP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-nHc7hB7wUF"
   },
   "source": [
    "# Getting the Dataset and EDA\n",
    "\n",
    "We will be working on S800 Corpus, which is a novel abstract-based manually annotated corpus. S800 comprises 800 PubMed abstracts in which organism mentions were identified and mapped to the corresponding NCBI Taxonomy identifiers.\n",
    "\n",
    "It is available on Hugging Face Datasets Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2017,
     "status": "ok",
     "timestamp": 1650629065195,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "oiFtTMyry0rK"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('species_800')\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train']).explode(['tokens', 'ner_tags']).dropna()\n",
    "valid_df = pd.DataFrame(dataset['validation']).explode(['tokens', 'ner_tags']).dropna()\n",
    "test_df = pd.DataFrame(dataset['test']).explode(['tokens', 'ner_tags']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650629065198,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "YwmEqJAxzTTj",
    "outputId": "0a2965db-a864-470c-e31d-c7a21aa344a0"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtzeA-Te9xAM"
   },
   "source": [
    "Here ner_tag 1 indicates B\n",
    "and ner_tag 2 indicates I\n",
    "while ner_tag 0 indicates O\n",
    "\n",
    "for simplicity we will combine B and I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1650629065918,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "5Y6SPkV5-CuX"
   },
   "outputs": [],
   "source": [
    "train_df['ner_tags'] = train_df['ner_tags'].apply(lambda x: 1 if x > 0 else 0)\n",
    "valid_df['ner_tags'] = valid_df['ner_tags'].apply(lambda x: 1 if x > 0 else 0)\n",
    "test_df['ner_tags'] = test_df['ner_tags'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhDHajh-9ewG"
   },
   "source": [
    "**Create list of list of tuples to differentiate each sentence from each other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650629065920,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "z9VjHDGU0JAL"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset, word_col, tag_col, sent_id_col):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [\n",
    "            (w, t) for w,t in zip(s[word_col].values.tolist(), s[tag_col].values.tolist())\n",
    "        ]\n",
    "        self.grouped = self.dataset.groupby(sent_id_col).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1650629066923,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "htPTQeiw1M9m",
    "outputId": "ea3b5019-3f58-4735-ed62-19764946be69"
   },
   "outputs": [],
   "source": [
    "train_getter = SentenceGetter(dataset=train_df, word_col='tokens', tag_col='ner_tags', sent_id_col='id')\n",
    "valid_getter = SentenceGetter(dataset=valid_df, word_col='tokens', tag_col='ner_tags', sent_id_col='id')\n",
    "test_getter = SentenceGetter(dataset=test_df, word_col='tokens', tag_col='ner_tags', sent_id_col='id')\n",
    "\n",
    "train_sentences = train_getter.sentences\n",
    "valid_sentences = valid_getter.sentences\n",
    "test_sentences = test_getter.sentences\n",
    "print('Sample Sentence')\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650629066925,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "9sbOeTzd9toq",
    "outputId": "5494ad1a-6a66-46fb-bfc3-5c1d755dd468"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "ax.hist([len(s) for s in train_sentences], bins=50)\n",
    "ax.set_title('Number of words in each Sentence')\n",
    "\n",
    "maxlen = max([len(s) for s in train_sentences])\n",
    "print('Number of Sentences:', len(train_sentences))\n",
    "print ('Maximum sequence length:', maxlen)\n",
    "\n",
    "words = list(set(train_df[\"tokens\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "words.append(\"UNK\")\n",
    "n_words = len(words)\n",
    "print('Number of unique words:', n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VuLnKpy_RMj"
   },
   "source": [
    "**Converting words to numbers and numbers to words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1650629066932,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "AyLf_F4N_QVi"
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for i, w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n6W0bOK_cxN"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1650629066935,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "qbnZPCG4_b7H",
    "tags": [
     "same_step_1"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtX7DfgiDy8u"
   },
   "source": [
    "**Lets first tokenize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1650629067651,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "xs3PVPxI_QSD",
    "outputId": "84d17554-0e4b-4961-8cfe-be5bc967d69d"
   },
   "outputs": [],
   "source": [
    "X_train = [[word2idx.get(w[0], len(word2idx)-1) for w in s] for s in train_sentences]\n",
    "X_train = sequence.pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\",value=n_words - 2)\n",
    "\n",
    "y_train = [[[w[1]] for w in s] for s in train_sentences]\n",
    "y_train = sequence.pad_sequences(maxlen=maxlen, sequences=y_train, padding=\"post\", value=0)\n",
    "y_train = np.array([to_categorical(i, num_classes=2) for i in y_train])\n",
    "\n",
    "print('X shape', X_train.shape, 'y shape', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1650629067652,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "gPdgQhvL_QPx",
    "outputId": "437f058d-2984-464f-b10c-da21b13004a0"
   },
   "outputs": [],
   "source": [
    "X_valid = [[word2idx.get(w[0], len(word2idx)-1) for w in s] for s in valid_sentences]\n",
    "X_valid = sequence.pad_sequences(maxlen=maxlen, sequences=X_valid, padding=\"post\",value=n_words - 2)\n",
    "\n",
    "y_valid = [[[w[1]] for w in s] for s in valid_sentences]\n",
    "y_valid = sequence.pad_sequences(maxlen=maxlen, sequences=y_valid, padding=\"post\", value=0)\n",
    "y_valid = np.array([to_categorical(i, num_classes=2) for i in y_valid])\n",
    "\n",
    "print('X shape', X_valid.shape, 'y shape', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650629067653,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "kq957YbLDUez",
    "outputId": "4dde00f5-68b6-4159-c63a-f9214d5c6ede"
   },
   "outputs": [],
   "source": [
    "X_test = [[word2idx.get(w[0], len(word2idx)-1) for w in s] for s in test_sentences]\n",
    "X_test = sequence.pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=n_words - 2)\n",
    "\n",
    "y_test = [[[w[1]] for w in s] for s in test_sentences]\n",
    "y_test = sequence.pad_sequences(maxlen=maxlen, sequences=y_test, padding=\"post\", value=0)\n",
    "y_test = np.array([to_categorical(i, num_classes=2) for i in y_test])\n",
    "\n",
    "print('X shape', X_test.shape, 'y shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbt_iTSKD_Xa"
   },
   "source": [
    "**Training Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1650629067654,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "TQNdDtcP_QNL"
   },
   "outputs": [],
   "source": [
    "class config():\n",
    "    VOCAB = n_words\n",
    "    MAX_LEN = maxlen\n",
    "    N_OUPUT = 2\n",
    "    \n",
    "    \n",
    "    EMBEDDING_VECTOR_LENGTH = 50\n",
    "    DENSE_DIM = 32\n",
    "    NUM_HEADS = 2\n",
    "    \n",
    "    OUTPUT_ACTIVATION = 'softmax'\n",
    "    \n",
    "    LOSS = 'categorical_crossentropy'\n",
    "    OPTIMIZER = 'adam'\n",
    "    METRICS = ['accuracy']\n",
    "    \n",
    "    MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0Vxu0RFEAyi"
   },
   "source": [
    "**Lets define a standard Transformer Encoder Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1650629067655,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "T3clfbxU_QKi"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(L.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([L.Dense(dense_dim, activation='relu'), L.Dense(embed_dim)])\n",
    "        self.layernorm1 = L.LayerNormalization()\n",
    "        self.layernorm2 = L.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[: tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_confog()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykTzN1aXEL1Y"
   },
   "source": [
    "**Some standard Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1650629067656,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "CabuC5rBEK7u"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
    "rlp = ReduceLROnPlateau(monitor='loss', patience=3)\n",
    "csv_logger = CSVLogger('training_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRCFw15DEPdv"
   },
   "source": [
    "**Lets define our model**\n",
    "\n",
    "A token classification is pretty simple and similar to that of sequence classification, ie there is only one change, since we need predictions for each input tokken we do not use the Global Pooling Layer, therefore the architechture looks something like:\n",
    "* Input Layer\n",
    "* Embeddings\n",
    "* Transformer Encoder Block\n",
    "* Dropout (optional)\n",
    "* Classification Layer (where n_units = number of classes, ie 2 in out case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1650629068345,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "0ylxJEnR_QHz",
    "outputId": "eea094aa-9ae1-4073-acd6-7fac27f5792f"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "x = L.Embedding(config.VOCAB, config.EMBEDDING_VECTOR_LENGTH)(inputs)\n",
    "x = TransformerEncoder(config.EMBEDDING_VECTOR_LENGTH, config.DENSE_DIM, config.NUM_HEADS)(x)\n",
    "x = L.Dropout(0.5)(x)\n",
    "outputs = L.Dense(config.N_OUPUT, activation=config.OUTPUT_ACTIVATION)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER, metrics=config.METRICS)\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 205137,
     "status": "ok",
     "timestamp": 1650629273469,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "FrR__pB1_QDS",
    "tags": [
     "same_step_2"
    ]
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    callbacks=[es, rlp, csv_logger], epochs=config.MAX_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 59477,
     "status": "ok",
     "timestamp": 1650629332929,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "BAF-NVehBusb",
    "outputId": "45d42de0-c10a-45ed-be3d-6bbe6b17314a"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 8))\n",
    "df = pd.DataFrame(history.history)\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=ax[0])\n",
    "df[['loss', 'val_loss']].plot(ax=ax[1])\n",
    "ax[0].set_title('Model Accuracy', fontsize=12)\n",
    "ax[1].set_title('Model Loss', fontsize=12)\n",
    "fig.suptitle('Model Metrics', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrAApw08EzwT"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1650629332940,
     "user": {
      "displayName": "Ritvik Rastogi",
      "userId": "01504835329516909297"
     },
     "user_tz": -330
    },
    "id": "BPJmHAtNBupE",
    "outputId": "f576c644-c1e5-4a17-c467-4638e04717d7"
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, X_test.shape[0])\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "y_true = np.argmax(y_test, axis=-1)[i]\n",
    "\n",
    "print(f\"{'Word':15}{'True':5}\\t{'Pred'}\")\n",
    "print(\"-\"*30)\n",
    "for (w, t, pred) in zip(X_test[i], y_true, p[0]):\n",
    "    print(f\"{words[w]:15}{t}\\t{pred}\")\n",
    "    if words[w] == 'ENDPAD': \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "authorship_tag": "ABX9TyN2EpdGIDLIB6rkOFF3pTD4",
   "collapsed_sections": [],
   "name": "Named-Emtity-Recognition-Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
