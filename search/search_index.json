{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/","text":"Welcome to the SAME Project \u00b6 The Issue with Jupyter Notebooks (Which We Love!) \u00b6 Data developers and data scientists are using Jupyter notebooks to build all manner of sophisticated data and machine learning pipelines. However, when it comes to deploying these notebooks into production or higher scale environments, most platforms do not \"speak Jupyter natively\". To accelerate data development, we need to collaborate to bridge this gap. We would like to propose that the larger Data Developer ecosystem needs: A unified, distributed-friendly method for converting the millions of notebooks already created to \"backend-friendly\", deployable artifacts. Stateless, GitOps-aware tooling that can connect to platforms seamlessly, and flexible enough to take advantage of each backend's unique features. A simple metadata format that enables reproducibility and portability, in local, in production and across organizational boundaries with as few changes to the developer experience as possible. Most of all, we need all this without giving up the tool that millions of developers know and love - Jupyter. Introducing the SAME Project \u00b6 The SAME project takes on the challenge of moving notebooks to production directly. We will give developers simple tools, via CLI, SDKs and IDE extensions, to enable building production-ready Jupyter notebooks with no/small changes to their existing workflows. The standard notebook developer won't have to learn the subtleties (or APIs) of a hosting platform - they use SAME tools & SDKs and \"it will just work\". The vision of the SAME Project is to include: A compiler that translates user code into workflow engine specs. An adapter framework that can interact with workflow engines. A portable, extensible CLI to execute actions. In-notebook tools and SDKs that help users author deployable code. (Optional) In launching our project, we have already made a great deal of progress towards this goal: With just what we've built so far, we believe we have the following benefits: Use the SAME tools to develop faster locally. Use the SAME tools to deploy to a distributed platform. Use the SAME tools across the application lifecycle and to collaborate. Our north star is a world in which developers will be able to build workflows more quickly and reliably with tools they are already familiar with. At the end of the day, developers will be able to: Write richer, reliable, and debuggable workflows. Move solutions to distributed deployments faster, solving business needs. Migrate solutions across environments, enabling both global-scale systems and collaboration. The SAME Community \u00b6 SAME is entirely open-source and non-commercial. We plan on donating it to a foundation as soon as we can identify one that matches our project's goals. What can you do? Please join our community! Public web content \u00b6 Website Google Group Slack Come join our repo \u00b6 GitHub Organization / GitHub Project Try it out (build instructions included) Complain about missing features EXPERTS ONLY: Add your own Regardless, we are very open to taking your feedback. Thank you so much - onward! -- The Co-founders of the SAME Project ( David Aronchick & Luke Marsden )","title":"Welcome to the SAME Project"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/#welcome-to-the-same-project","text":"","title":"Welcome to the SAME Project"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/#the-issue-with-jupyter-notebooks-which-we-love","text":"Data developers and data scientists are using Jupyter notebooks to build all manner of sophisticated data and machine learning pipelines. However, when it comes to deploying these notebooks into production or higher scale environments, most platforms do not \"speak Jupyter natively\". To accelerate data development, we need to collaborate to bridge this gap. We would like to propose that the larger Data Developer ecosystem needs: A unified, distributed-friendly method for converting the millions of notebooks already created to \"backend-friendly\", deployable artifacts. Stateless, GitOps-aware tooling that can connect to platforms seamlessly, and flexible enough to take advantage of each backend's unique features. A simple metadata format that enables reproducibility and portability, in local, in production and across organizational boundaries with as few changes to the developer experience as possible. Most of all, we need all this without giving up the tool that millions of developers know and love - Jupyter.","title":"The Issue with Jupyter Notebooks (Which We Love!)"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/#introducing-the-same-project","text":"The SAME project takes on the challenge of moving notebooks to production directly. We will give developers simple tools, via CLI, SDKs and IDE extensions, to enable building production-ready Jupyter notebooks with no/small changes to their existing workflows. The standard notebook developer won't have to learn the subtleties (or APIs) of a hosting platform - they use SAME tools & SDKs and \"it will just work\". The vision of the SAME Project is to include: A compiler that translates user code into workflow engine specs. An adapter framework that can interact with workflow engines. A portable, extensible CLI to execute actions. In-notebook tools and SDKs that help users author deployable code. (Optional) In launching our project, we have already made a great deal of progress towards this goal: With just what we've built so far, we believe we have the following benefits: Use the SAME tools to develop faster locally. Use the SAME tools to deploy to a distributed platform. Use the SAME tools across the application lifecycle and to collaborate. Our north star is a world in which developers will be able to build workflows more quickly and reliably with tools they are already familiar with. At the end of the day, developers will be able to: Write richer, reliable, and debuggable workflows. Move solutions to distributed deployments faster, solving business needs. Migrate solutions across environments, enabling both global-scale systems and collaboration.","title":"Introducing the SAME Project"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/#the-same-community","text":"SAME is entirely open-source and non-commercial. We plan on donating it to a foundation as soon as we can identify one that matches our project's goals. What can you do? Please join our community!","title":"The SAME Community"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/#public-web-content","text":"Website Google Group Slack","title":"Public web content"},{"location":"blog/2022-06-15-Introducing-The-SAME-Project/#come-join-our-repo","text":"GitHub Organization / GitHub Project Try it out (build instructions included) Complain about missing features EXPERTS ONLY: Add your own Regardless, we are very open to taking your feedback. Thank you so much - onward! -- The Co-founders of the SAME Project ( David Aronchick & Luke Marsden )","title":"Come join our repo"},{"location":"getting-started/adding-steps/","text":"In order to split a notebook into multiple pipeline steps, we add cell tags in the notebook. Option 1: Jupyter Lab \u00b6 Add a Tag that Specifies a Step \u00b6 Go to the cell you want to split steps with, and click the \"cogs\" icon in the top right. Add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.) Option 2: Classic Jupyter \u00b6 Allowing for Adding Specific Tags \u00b6 First, go to the notebook settings and view \"Cell Metadata\": Add a Tag that Specifies a Step \u00b6 Second, go to the cell you want to split steps with, and add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.) Execute the Steps \u00b6 Now, when you execute same run the cells will automatically be grouped together into steps, and executed serially. No additional work is necessary for the end user. We inject an additional step for each graph, a \"run info\" step (which is necessary because we only know things like Run ID when the step is executed).","title":"Adding Steps"},{"location":"getting-started/adding-steps/#option-1-jupyter-lab","text":"","title":"Option 1: Jupyter Lab"},{"location":"getting-started/adding-steps/#add-a-tag-that-specifies-a-step","text":"Go to the cell you want to split steps with, and click the \"cogs\" icon in the top right. Add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.)","title":"Add a Tag that Specifies a Step"},{"location":"getting-started/adding-steps/#option-2-classic-jupyter","text":"","title":"Option 2: Classic Jupyter"},{"location":"getting-started/adding-steps/#allowing-for-adding-specific-tags","text":"First, go to the notebook settings and view \"Cell Metadata\":","title":"Allowing for Adding Specific Tags"},{"location":"getting-started/adding-steps/#add-a-tag-that-specifies-a-step_1","text":"Second, go to the cell you want to split steps with, and add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.)","title":"Add a Tag that Specifies a Step"},{"location":"getting-started/adding-steps/#execute-the-steps","text":"Now, when you execute same run the cells will automatically be grouped together into steps, and executed serially. No additional work is necessary for the end user. We inject an additional step for each graph, a \"run info\" step (which is necessary because we only know things like Run ID when the step is executed).","title":"Execute the Steps"},{"location":"getting-started/changing-environment/","text":"Often times, a specific step will require a particular base container image to execute inside of. This may be because the image was pre-built with particular dependencies (packages, libraries, drivers, etc) which Jupyter does not need to execute, but may be necessary on the back end. This feature allows to specify the base image required to execute a given step. Adding an Environment Specifier \u00b6 Editing cell tags in the same way you did for adding steps , you can add alternate base images with the following tag structure: environment = environment_name For example: environment = default or: environment = private-training-env Update SAME Config \u00b6 Then in your same.yaml config file, you'll add a section that maps to the specific image you'll need.","title":"Changing Environment"},{"location":"getting-started/changing-environment/#adding-an-environment-specifier","text":"Editing cell tags in the same way you did for adding steps , you can add alternate base images with the following tag structure: environment = environment_name For example: environment = default or: environment = private-training-env","title":"Adding an Environment Specifier"},{"location":"getting-started/changing-environment/#update-same-config","text":"Then in your same.yaml config file, you'll add a section that maps to the specific image you'll need.","title":"Update SAME Config"},{"location":"getting-started/dev-build/","text":"Prerequisites \u00b6 Python 3.8+ Poetry 1.1.7 or higher Clone the repo to your local machine and initialize the submodules: git clone https://github.com/SAME-Project/same-project.git cd same-project git submodule update --init --recursive Download and install Poetry, which is used to manage dependencies and virtual environments for the SAME project. You will need to install the project's Python dependencies using Poetry as well after installing it: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python3 - poetry install To install AML dependencies, now optional, use poetry install --extras azureml Using the repo \u00b6 Use of the SAME python project assumes executing in a virtual environment managed by Poetry. Before running any commands, the virtual environment should be started: poetry shell NOTE: From this point forward, all functions require executing inside that virtual environment. If you see an error like zsh: command not found , it could be because you're not executing inside the venv. You can check this by executing: which python3 This should result in a response like: .../pypoetry/virtualenvs/same-project-88mixeKa-py3.8/bin/python3 . If it reports something like /usr/bin/python or /usr/local/bin/python , you are using the system python, and things will not work as expected. How to execute against a notebook from source code \u00b6 From the root of project, execute: same <cli-arguments> Running tests \u00b6 To run all the tests against the CLI and SDK: pytest To run a subset of tests for a single file: pytest test/cli/test_<file>.py -k \"test_<name>\" How to setup private test environments \u00b6 Local Kubeflow cluster on Minikube \u00b6 You can set up a local Kubeflow cluster to run the CLI pytests against if you wish: Start a minikube cluster in the devcontainer: Note: Kubeflow currently defines its Custom Resource Definitions (CRD) under apiextensions.k8s.io/v1beta which is deprecated in Kubernetes v1.22, so minikube must start the cluster with a version <1.22. See kubeflow/kfctl issue #500 . minikube start --kubernetes-version = v1.21.5 Starting minikube will also change the default kubeconfig context to the minikube cluster. You can check this with: kubectl config get-contexts Deploy Kubeflow to the minikube cluster: export PIPELINE_VERSION = 1 .7.0 kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= $PIPELINE_VERSION \" kubectl wait --for condition = established --timeout = 60s crd/applications.app.k8s.io kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= $PIPELINE_VERSION \" Kubeflow cluster on Azure Kubernetes Services (AKS) \u00b6 From any Azure subscription where you are at least a Contributor, you can create and provision a new AKS cluster with Kubeflow: Create a new AKS cluster either using the Azure CLI or Azure Portal . The linked instructions will also update your kubeconfig to use the new cluster as the context when you run az aks get-credentials , but you can also manually do so with: kubectl config set-context <context name> Deploy Kubeflow to the cluster. Note: The document references a non-existent v1.3.0 release, you can simply use the v1.2.0 release instead. See kubeflow/kfctl issue #495 . Azure Machine Learning (AML) workspace and compute \u00b6 Create a new Service Principal for running tests against your private AML instance. As mentioned in the instructions, make sure to take note of the output of the command as you will need the clientId , clientSecret , and tenantId values to configure the .env.sh file to run the AML tests. Create a new Azure Machine Learning Workspace . You will need the --resource-group and --workspace-name values you specified during workspace creation to configure the .env.sh file to run the AML tests. You will also need the subscription id that you created the AML workspace in. You can check this by running: az account show --query id Create an AML Compute cluster or AML Compute Instance . You will need the --name that you specified during compute cluster/instance creation to configure the .env.sh file to run the AML tests.","title":"Setting Up a Development Environment"},{"location":"getting-started/dev-build/#prerequisites","text":"Python 3.8+ Poetry 1.1.7 or higher Clone the repo to your local machine and initialize the submodules: git clone https://github.com/SAME-Project/same-project.git cd same-project git submodule update --init --recursive Download and install Poetry, which is used to manage dependencies and virtual environments for the SAME project. You will need to install the project's Python dependencies using Poetry as well after installing it: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python3 - poetry install To install AML dependencies, now optional, use poetry install --extras azureml","title":"Prerequisites"},{"location":"getting-started/dev-build/#using-the-repo","text":"Use of the SAME python project assumes executing in a virtual environment managed by Poetry. Before running any commands, the virtual environment should be started: poetry shell NOTE: From this point forward, all functions require executing inside that virtual environment. If you see an error like zsh: command not found , it could be because you're not executing inside the venv. You can check this by executing: which python3 This should result in a response like: .../pypoetry/virtualenvs/same-project-88mixeKa-py3.8/bin/python3 . If it reports something like /usr/bin/python or /usr/local/bin/python , you are using the system python, and things will not work as expected.","title":"Using the repo"},{"location":"getting-started/dev-build/#how-to-execute-against-a-notebook-from-source-code","text":"From the root of project, execute: same <cli-arguments>","title":"How to execute against a notebook from source code"},{"location":"getting-started/dev-build/#running-tests","text":"To run all the tests against the CLI and SDK: pytest To run a subset of tests for a single file: pytest test/cli/test_<file>.py -k \"test_<name>\"","title":"Running tests"},{"location":"getting-started/dev-build/#how-to-setup-private-test-environments","text":"","title":"How to setup private test environments"},{"location":"getting-started/dev-build/#local-kubeflow-cluster-on-minikube","text":"You can set up a local Kubeflow cluster to run the CLI pytests against if you wish: Start a minikube cluster in the devcontainer: Note: Kubeflow currently defines its Custom Resource Definitions (CRD) under apiextensions.k8s.io/v1beta which is deprecated in Kubernetes v1.22, so minikube must start the cluster with a version <1.22. See kubeflow/kfctl issue #500 . minikube start --kubernetes-version = v1.21.5 Starting minikube will also change the default kubeconfig context to the minikube cluster. You can check this with: kubectl config get-contexts Deploy Kubeflow to the minikube cluster: export PIPELINE_VERSION = 1 .7.0 kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= $PIPELINE_VERSION \" kubectl wait --for condition = established --timeout = 60s crd/applications.app.k8s.io kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= $PIPELINE_VERSION \"","title":"Local Kubeflow cluster on Minikube"},{"location":"getting-started/dev-build/#kubeflow-cluster-on-azure-kubernetes-services-aks","text":"From any Azure subscription where you are at least a Contributor, you can create and provision a new AKS cluster with Kubeflow: Create a new AKS cluster either using the Azure CLI or Azure Portal . The linked instructions will also update your kubeconfig to use the new cluster as the context when you run az aks get-credentials , but you can also manually do so with: kubectl config set-context <context name> Deploy Kubeflow to the cluster. Note: The document references a non-existent v1.3.0 release, you can simply use the v1.2.0 release instead. See kubeflow/kfctl issue #495 .","title":"Kubeflow cluster on Azure Kubernetes Services (AKS)"},{"location":"getting-started/dev-build/#azure-machine-learning-aml-workspace-and-compute","text":"Create a new Service Principal for running tests against your private AML instance. As mentioned in the instructions, make sure to take note of the output of the command as you will need the clientId , clientSecret , and tenantId values to configure the .env.sh file to run the AML tests. Create a new Azure Machine Learning Workspace . You will need the --resource-group and --workspace-name values you specified during workspace creation to configure the .env.sh file to run the AML tests. You will also need the subscription id that you created the AML workspace in. You can check this by running: az account show --query id Create an AML Compute cluster or AML Compute Instance . You will need the --name that you specified during compute cluster/instance creation to configure the .env.sh file to run the AML tests.","title":"Azure Machine Learning (AML) workspace and compute"},{"location":"getting-started/example-notebook/","text":"Clone the Sample Repo \u00b6 To get started, first you'll need to clone our sample repo. To do so, execute the following command: git clone https://github.com/SAME-Project/SAME-samples.git Next, change into the 03-road-signs directory: cd SAME-samples cd 03 -road-signs This is a complete data fetching, data engineering and model training example in three steps. Check the Requirements Work With the Container and the Notebook \u00b6 This step is optional, and requires that you have Docker running locally. If you don't, you can skip straight to deploying the pipeline to your Workflow Engine. same verify Note: this step can fail spuriously when run on a different architecture versus the workflow engine (e.g. M1 Mac). Deploy Pipeline to Workflow Engine \u00b6 Finally, deploy your notebook to your environment. If you are using Kubeflow, you would execute the following command: same run This command converts the notebook into a multi-stage pipeline, and deploys it to your previously configured Kubeflow. In the Kubeflow UI, click on Pipelines -> Experiments to see your runs! Try Your Own Notebook \u00b6 Try running same init And then same verify And finally same run In an empty folder with your own notebook in it! If you have any issues, please report a GitHub issue or come tell us on Slack !","title":"Hello World Notebook"},{"location":"getting-started/example-notebook/#clone-the-sample-repo","text":"To get started, first you'll need to clone our sample repo. To do so, execute the following command: git clone https://github.com/SAME-Project/SAME-samples.git Next, change into the 03-road-signs directory: cd SAME-samples cd 03 -road-signs This is a complete data fetching, data engineering and model training example in three steps.","title":"Clone the Sample Repo"},{"location":"getting-started/example-notebook/#check-the-requirements-work-with-the-container-and-the-notebook","text":"This step is optional, and requires that you have Docker running locally. If you don't, you can skip straight to deploying the pipeline to your Workflow Engine. same verify Note: this step can fail spuriously when run on a different architecture versus the workflow engine (e.g. M1 Mac).","title":"Check the Requirements Work With the Container and the Notebook"},{"location":"getting-started/example-notebook/#deploy-pipeline-to-workflow-engine","text":"Finally, deploy your notebook to your environment. If you are using Kubeflow, you would execute the following command: same run This command converts the notebook into a multi-stage pipeline, and deploys it to your previously configured Kubeflow. In the Kubeflow UI, click on Pipelines -> Experiments to see your runs!","title":"Deploy Pipeline to Workflow Engine"},{"location":"getting-started/example-notebook/#try-your-own-notebook","text":"Try running same init And then same verify And finally same run In an empty folder with your own notebook in it! If you have any issues, please report a GitHub issue or come tell us on Slack !","title":"Try Your Own Notebook"},{"location":"getting-started/installing/","text":"Installing SAME \u00b6 System Requirements \u00b6 Python 3.8+ Docker (optional, used by same verify command) Install sameproject from PyPI \u00b6 For example, with pip pip3 install --upgrade sameproject Verify installation \u00b6 Validate successful installation by running same version . Output should look similar to below same version 0.2.2 Connecting to a Workflow Engine \u00b6 To run SAME, you will need a workflow engine to connect to. We support Kubeflow Pipelines. Option A: Test Drive \u00b6 Use a test drive Kubernetes cluster with Kubeflow Pipelines preinstalled. This test drive cluster will expire 1 hour after starting it. You can run testctl get again to get a new one at any time. Register for a Testfaster account . Copy and run the testctl install instructions at Access Token . Make sure to include the testctl login command . Clone this Kubeflow Combinator repo and get a cluster: git clone https://github.com/combinator-ml/terraform-k8s-stack-kubeflow-mlflow cd terraform-k8s-stack-kubeflow-mlflow cd examples/testfaster testctl get export KUBECONFIG = $( pwd ) /kubeconfig (If you use a non- bash shell, you may need to spell the command to set an environment variable differently.) To launch the Kubeflow Pipelines UI, from the terraform-k8s-kubeflow/testfaster directory, run: testctl ip And copy the final URL into your browser. Log in with admin@kubeflow.org and 12341234 . Go to Pipelines -> Experiments -> Default in the Kubeflow UI. Now in the same shell you ran the export command, you can run same run (see next section) and it will be able to find and deploy to Kubeflow pipelines in the configured cluster. Option B: Use Existing Kubeflow Pipelines Install \u00b6 Ensure your active kubectl context is pointing to the Kubernetes cluster with Kubeflow Pipelines installed: kubectl config current-context If so, you are ready to run same run (see next page for example). After running same run , look in Pipelines -> Experiments -> Default in the Kubeflow UI. Option C: Install Kubeflow Pipelines on Kubernetes \u00b6 Follow the Kubeflow installation instructions , and then follow the existing Kubeflow Pipelines section. Or, try the Test Drive if this is too much hard work. Next \u00b6 You're done setting up SAME and are now ready to execute! The default execution uses Kubeflow (either locally or in the cloud). Use the -t flag to set another target. To try out an example, check out the roadsigns example from Example Notebook .","title":"Installing SAME"},{"location":"getting-started/installing/#installing-same","text":"","title":"Installing SAME"},{"location":"getting-started/installing/#system-requirements","text":"Python 3.8+ Docker (optional, used by same verify command)","title":"System Requirements"},{"location":"getting-started/installing/#install-sameproject-from-pypi","text":"For example, with pip pip3 install --upgrade sameproject","title":"Install sameproject from PyPI"},{"location":"getting-started/installing/#verify-installation","text":"Validate successful installation by running same version . Output should look similar to below same version 0.2.2","title":"Verify installation"},{"location":"getting-started/installing/#connecting-to-a-workflow-engine","text":"To run SAME, you will need a workflow engine to connect to. We support Kubeflow Pipelines.","title":"Connecting to a Workflow Engine"},{"location":"getting-started/installing/#option-a-test-drive","text":"Use a test drive Kubernetes cluster with Kubeflow Pipelines preinstalled. This test drive cluster will expire 1 hour after starting it. You can run testctl get again to get a new one at any time. Register for a Testfaster account . Copy and run the testctl install instructions at Access Token . Make sure to include the testctl login command . Clone this Kubeflow Combinator repo and get a cluster: git clone https://github.com/combinator-ml/terraform-k8s-stack-kubeflow-mlflow cd terraform-k8s-stack-kubeflow-mlflow cd examples/testfaster testctl get export KUBECONFIG = $( pwd ) /kubeconfig (If you use a non- bash shell, you may need to spell the command to set an environment variable differently.) To launch the Kubeflow Pipelines UI, from the terraform-k8s-kubeflow/testfaster directory, run: testctl ip And copy the final URL into your browser. Log in with admin@kubeflow.org and 12341234 . Go to Pipelines -> Experiments -> Default in the Kubeflow UI. Now in the same shell you ran the export command, you can run same run (see next section) and it will be able to find and deploy to Kubeflow pipelines in the configured cluster.","title":"Option A: Test Drive"},{"location":"getting-started/installing/#option-b-use-existing-kubeflow-pipelines-install","text":"Ensure your active kubectl context is pointing to the Kubernetes cluster with Kubeflow Pipelines installed: kubectl config current-context If so, you are ready to run same run (see next page for example). After running same run , look in Pipelines -> Experiments -> Default in the Kubeflow UI.","title":"Option B: Use Existing Kubeflow Pipelines Install"},{"location":"getting-started/installing/#option-c-install-kubeflow-pipelines-on-kubernetes","text":"Follow the Kubeflow installation instructions , and then follow the existing Kubeflow Pipelines section. Or, try the Test Drive if this is too much hard work.","title":"Option C: Install Kubeflow Pipelines on Kubernetes"},{"location":"getting-started/installing/#next","text":"You're done setting up SAME and are now ready to execute! The default execution uses Kubeflow (either locally or in the cloud). Use the -t flag to set another target. To try out an example, check out the roadsigns example from Example Notebook .","title":"Next"},{"location":"getting-started/working-with-datasets/","text":"You may wish to use different datasets in your notebook depending on which environment it is deployed to. The sameproject SDK allows you to configure environment -specific datasets in your SAME config file, and load them in a uniform way inside your notebook. Add the SDK as a Requirement \u00b6 First, you will need to add sameproject to your configured requirements.txt file: tensorflow == 2 .8.0 [ ... ] sameproject == 0 .2.2 Update SAME Config \u00b6 Next, configure environment-specific datasets in the datasets section of your same.yaml config file: Note that SAME supports local files and web URLs as well as the IPFS protocol. Load the Dataset in your Notebook \u00b6 Finally, you can load your dataset in an environment-specific way using the sameproject.sdk module in your notebook: When you deploy your notebook to an execution backend using same run , the environment that has been specified either through notebook tags or the --same-env flag will be used to load the appropriate dataset in your notebook: same run --same-env = staging","title":"Working with Datasets"},{"location":"getting-started/working-with-datasets/#add-the-sdk-as-a-requirement","text":"First, you will need to add sameproject to your configured requirements.txt file: tensorflow == 2 .8.0 [ ... ] sameproject == 0 .2.2","title":"Add the SDK as a Requirement"},{"location":"getting-started/working-with-datasets/#update-same-config","text":"Next, configure environment-specific datasets in the datasets section of your same.yaml config file: Note that SAME supports local files and web URLs as well as the IPFS protocol.","title":"Update SAME Config"},{"location":"getting-started/working-with-datasets/#load-the-dataset-in-your-notebook","text":"Finally, you can load your dataset in an environment-specific way using the sameproject.sdk module in your notebook: When you deploy your notebook to an execution backend using same run , the environment that has been specified either through notebook tags or the --same-env flag will be used to load the appropriate dataset in your notebook: same run --same-env = staging","title":"Load the Dataset in your Notebook"},{"location":"getting-started/platforms/setting-up-azure-functions/","text":"The functions backend is an Azure Durable Functions app that can execute SAME runs. In order to use it, you will first need to deploy the app to an Azure subscription that you own, and then configure SAME to send requests to it. Prerequisites \u00b6 You will need the following tools to deploy the functions backend: az , the Azure CLI tool. func , the Azure Functions CLI tool. terraform , the Terraform CLI tool. Initial Setup \u00b6 First, clone the same-project git repository: git clone https://github.com/SAME-Project/same-project.git cd same-project Next, you will need to authorise terraform to deploy resources to your Azure account. This can either be done using your personal user account , or by using a service principal you have created for this purpose. Apply the Terraform \u00b6 The functions backend provides Terraform scripts for provisioning the Azure resources it needs: cd sameproject/ops/functions/terraform terraform init terraform apply Once terraform has finished provisioning the resources, take note of the name and hostname of the function app that was created: export FUNCTIONS_APP_NAME = $( terraform output -raw app_name ) export FUNCTIONS_HOST_NAME = $( terraform output -raw host_name ) Deploy the Functions App \u00b6 Next, you will need to deploy the SAME functions backend to your newly provisioned resources: cd - && cd sameproject/ops/functions/functions func azure functionapp publish $FUNCTIONS_APP_NAME Once the deployment has finished you should be able to see the functions in the Azure Functions portal : The portal allows you to see which functions are active in the function app: Clicking on one of these functions will bring up some useful tools for debugging and monitoring the app: Test the Deployment \u00b6 You are now ready to execute SAME runs on Azure Functions! To test your deployment, you can run one of the test suite notebooks in the same-project repository: cd - && cd test/testdata/features/singlestep same run -t functions -f same.yaml \\ --functions-host-name \" ${ FUNCTIONS_HOST_NAME } \" Limitations \u00b6 The Azure Functions backend does not support custom Docker images in SAME config files. All python dependencies must be specified using a requirements.txt , and non-python dependencies are not currently supported.","title":"Setting up Azure Functions"},{"location":"getting-started/platforms/setting-up-azure-functions/#prerequisites","text":"You will need the following tools to deploy the functions backend: az , the Azure CLI tool. func , the Azure Functions CLI tool. terraform , the Terraform CLI tool.","title":"Prerequisites"},{"location":"getting-started/platforms/setting-up-azure-functions/#initial-setup","text":"First, clone the same-project git repository: git clone https://github.com/SAME-Project/same-project.git cd same-project Next, you will need to authorise terraform to deploy resources to your Azure account. This can either be done using your personal user account , or by using a service principal you have created for this purpose.","title":"Initial Setup"},{"location":"getting-started/platforms/setting-up-azure-functions/#apply-the-terraform","text":"The functions backend provides Terraform scripts for provisioning the Azure resources it needs: cd sameproject/ops/functions/terraform terraform init terraform apply Once terraform has finished provisioning the resources, take note of the name and hostname of the function app that was created: export FUNCTIONS_APP_NAME = $( terraform output -raw app_name ) export FUNCTIONS_HOST_NAME = $( terraform output -raw host_name )","title":"Apply the Terraform"},{"location":"getting-started/platforms/setting-up-azure-functions/#deploy-the-functions-app","text":"Next, you will need to deploy the SAME functions backend to your newly provisioned resources: cd - && cd sameproject/ops/functions/functions func azure functionapp publish $FUNCTIONS_APP_NAME Once the deployment has finished you should be able to see the functions in the Azure Functions portal : The portal allows you to see which functions are active in the function app: Clicking on one of these functions will bring up some useful tools for debugging and monitoring the app:","title":"Deploy the Functions App"},{"location":"getting-started/platforms/setting-up-azure-functions/#test-the-deployment","text":"You are now ready to execute SAME runs on Azure Functions! To test your deployment, you can run one of the test suite notebooks in the same-project repository: cd - && cd test/testdata/features/singlestep same run -t functions -f same.yaml \\ --functions-host-name \" ${ FUNCTIONS_HOST_NAME } \"","title":"Test the Deployment"},{"location":"getting-started/platforms/setting-up-azure-functions/#limitations","text":"The Azure Functions backend does not support custom Docker images in SAME config files. All python dependencies must be specified using a requirements.txt , and non-python dependencies are not currently supported.","title":"Limitations"},{"location":"getting-started/platforms/setting-up-azureml/","text":"We recommend starting on Azure with the Azure ML Terraform setup In order to use AzureML, you need to have environment variables named the following: export AML_SP_PASSWORD_VALUE = export AML_SP_TENANT_ID = export AML_SP_APP_ID = export WORKSPACE_SUBSCRIPTION_ID = export WORKSPACE_RESOURCE_GROUP = export WORKSPACE_NAME = export AML_COMPUTE_NAME = To set the workspace variables, follow this instruction: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace Create a service principal account: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication az ad sp create-for-rbac --sdk-auth --name same-project-aml-auth --role Contributor --scopes /subscriptions/72ac7288-fb92-4ad6-83bc-5cfd361f47ef AML_SP_APP_ID => clientId AML_SP_PASSWORD_VALUE => clientSecret AML_SP_TENANT_ID => tenantId","title":"Setting up Azure ML"},{"location":"getting-started/platforms/setting-up-kubeflow/","text":"First, you will need a Kubernetes setup. You can test this by executing a simple command: kubectl get nodes Next, you will need to setup the default Kubeflow installation. We are following the manifests effort in Kubeflow (as of June 2022). KFP_ENV = platform-agnostic kubectl apply -k cluster-scoped-resources/ kubectl wait crd/applications.app.k8s.io --for condition = established --timeout = 60s kubectl apply -k \"env/ ${ KFP_ENV } /\" kubectl wait pods -l application-crd-id = kubeflow-pipelines -n kubeflow --for condition = Ready --timeout = 1800s You can test your installation is working properly by executing the following: kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080 :80 From this point, the SAME project should be up and ready to go!","title":"Setting up Kubeflow"},{"location":"getting-started/platforms/setting-up-pachyderm/","text":"Assuming you have a Pachyderm installation and pachctl list repo is working for you, deploying a notebook as a pipeline to Pachyderm is as simple as: Install SAME: pip3 install --upgrade sameproject Set up a same.yaml and requirements.txt in a folder alongside your .ipynb file: same init Test the suggested container image against the requirements.txt and your notebook's imports (optional, requires Docker): same verify Deploy the notebook as a pipeline to Pachyderm: same run --target pachyderm --input-repo test Update the input repo to refer to a repo that exists on your Pachyderm installation. You can also specify --input-glob to specify a glob pattern, or --input to specify a raw input specification in JSON format, to specify more advanced input formats. Your notebook should read data from /pfs , and write any output data to /pfs/out . You might want to use the Pachyderm JupyterLab Mount Extension to develop your notebook, then it will run the same way with the mount extension as when you run it in Pachyderm with SAME.","title":"Pachyderm"},{"location":"getting-started/platforms/setting-up-vertex/","text":"To begin with Vertex, first, you need to set up the environment. The script below will walk through the steps: # Set a series of enviornment variables export PROJECT_ID = \"xxx\" export SERVICE_ACCOUNT_ID = \"xxx\" export USER_EMAIL = \"xxx\" export BUCKET_NAME = \"xxx\" export FILE_NAME = \"xxx\" export GOOGLE_APPLICATION_CREDENTIALS = \"xxx\" gcloud iam service-accounts create $SERVICE_ACCOUNT_ID \\ --description = \"Service principal for running vertex and creating pipelines/metadata\" \\ --display-name = \" $SERVICE_ACCOUNT_ID \" \\ --project ${ PROJECT_ID } gcloud projects add-iam-policy-binding ${ PROJECT_ID } \\ --member = \"serviceAccount: $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com\" \\ --role = roles/storage.objectAdmin gcloud projects add-iam-policy-binding ${ PROJECT_ID } \\ --member = \"serviceAccount: $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com\" \\ --role = roles/aiplatform.user gcloud projects add-iam-policy-binding ${ PROJECT_ID } \\ --member = \"serviceAccount: $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com\" \\ --role = roles/ml.admin gcloud projects get-iam-policy $PROJECT_ID \\ --flatten = \"bindings[].members\" \\ --format = 'table(bindings.role)' \\ --filter = \"bindings.members:serviceAccount: $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com\" gcloud iam service-accounts add-iam-policy-binding \\ $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com \\ --member = \"user: $USER_EMAIL \" \\ --role = \"roles/iam.serviceAccountUser\" --project ${ PROJECT_ID } gsutil mb -p $PROJECT_ID gs:// $BUCKET_NAME gsutil iam ch \\ serviceAccount: $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com:roles/storage.objectCreator,objectViewer \\ gs:// $BUCKET_NAME gcloud iam service-accounts keys create $FILE_NAME .json --iam-account = $SERVICE_ACCOUNT_ID @ $PROJECT_ID .iam.gserviceaccount.com From this point, the SAME project should be ready to go!","title":"Setting up Vertex"}]}